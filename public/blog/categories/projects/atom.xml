<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Projects | Charles Hooper]]></title>
  <link href="http://www.charleshooper.net/blog/categories/projects/atom.xml" rel="self"/>
  <link href="http://www.charleshooper.net/"/>
  <updated>2015-05-24T07:49:52-07:00</updated>
  <id>http://www.charleshooper.net/</id>
  <author>
    <name><![CDATA[Charles Hooper]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[What I Do as an SRE]]></title>
    <link href="http://www.charleshooper.net/blog/what-i-do-sre/"/>
    <updated>2014-06-21T06:02:00-07:00</updated>
    <id>http://www.charleshooper.net/blog/what-i-do-as-an-sre</id>
    <content type="html"><![CDATA[<h1 id="section"></h1>

<p>Sometimes people ask me what I do and I’m not really sure how to answer
them. My answer tends to depend on social setting, what I’ve been
working on, and if I was on call that week. No matter the circumstances,
it usually comes out pretty boring and terribly short.</p>

<p>This really sucks though, because I actually really <em>like</em> my job and
think that it’s interesting, if only I could articulate it.</p>

<p>So here’s some attempt at explaining what I do:</p>

<ul>
  <li>
    <p>I’m an SRE, or <em>Service Reliability Engineer</em>,
at <a href="https://www.heroku.com/">Heroku</a>. Typically, SRE stands for <em>Site</em>
Reliability Engineer, however we’ve modernized it at Heroku because what
is even a site anymore?</p>
  </li>
  <li>
    <p>My week-to-week is wildly unpredictable. This week I’m conducting an
operational review of one of our key platform components, last week I
was investigating and addressing database bloat, and the week before I
was the on-call incident commander and quite busy due to several
incidents that occurred.</p>
  </li>
  <li>
    <p>Speaking of the incident commander role, part of my job includes
<em>defining</em> how we respond to incidents. At first glance it seems easy:
Get paged and show up. And then you respond to your first 24-hour
slow-burning incident and realize that you’ve got more work to do.</p>
  </li>
  <li>
    <p>Following incidents, I also schedule and faciliate retrospectives. We
practice <a href="http://codeascraft.com/2012/05/22/blameless-postmortems/">blameless
postmortems</a>
and these tend to be incredibly constructive.</p>
  </li>
  <li>
    <p>I also analyze past incident data and look for patterns and trends.
Wondering if there’s a day of week that has a higher probability of
experiencing an incident? Yeah, it’s Friday.</p>
  </li>
  <li>
    <p>When all is quiet, I review dashboards and investigate anomolies.
Wondering what that weird spike or dip is that seems to happen every
once in a while? Ask me, I’ve probably pulled that thread before (and if
I haven’t, I’ll be terribly curious).</p>
  </li>
  <li>
    <p>And sometimes I build integration tests and tools. I wrote
<a href="/blog/troubleshooting-elbs-with-elbping/">elbping</a>, for instance,
because ELBs were terrible to troubleshoot during an incident.</p>
  </li>
  <li>
    <p>And, most importantly, I mentor other SREs and software engineers.
This is the single biggest thing I can do in terms of its impact, and
probably most rewarding, too.</p>
  </li>
</ul>

<p>So there you have it, that’s what I do.</p>

<p>P.S. - If this sounds interesting to you, <a href="http://jobs.heroku.com/">we’re hiring</a>!</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Troubleshooting ELBs with elbping]]></title>
    <link href="http://www.charleshooper.net/blog/troubleshooting-elbs-with-elbping/"/>
    <updated>2013-10-19T10:49:00-07:00</updated>
    <id>http://www.charleshooper.net/blog/troubleshooting-elbs-with-elbping</id>
    <content type="html"><![CDATA[<h1 id="section"></h1>

<p>Troubleshooting ELBs can be pretty painful at times because they are
largely a black box. There aren’t many metrics available, and the ones
that do exist are aggregated across all of the nodes of an ELB. This can
be troublesome at times, for example when only a subset of an ELB’s
nodes are degraded.</p>

<h1 id="elb-properties">ELB Properties</h1>

<p>ELBs have some interesting properties. For instance:</p>

<ul>
  <li>ELBs are made up of 1 or more nodes</li>
  <li>These nodes are published as A records for the ELB name</li>
  <li>These nodes can fail, or be shut down, and connections will <em>not</em> be closed gracefully</li>
  <li>It often requires a good relationship with Amazon support ($$$) to get someone to dig into ELB problems</li>
</ul>

<p><em>NOTE: Another interesting property but slightly less pertinent is that
ELBs were not designed to handle sudden spikes of traffic. They
typically require 15 minutes of heavy traffic before they will scale up
or they can be pre-warmed on request via a support ticket</em></p>

<h1 id="troubleshooting-elbs-manually">Troubleshooting ELBs (manually)</h1>

<p><strong>Update:</strong> <em>Since writing this blog post, AWS has since migrated all
ELBs to use Route 53 for DNS. In addition, all ELBs now have a
<code>all.$elb_name</code> record that will return the full list of nodes for the
ELB. For example, if your ELB name is
<code>elb-123456789.us-east-1.elb.amazonaws.com</code>, then you would get the full
list of nodes by doing something like <code>dig
all.elb-123456789.us-east-1.elb.amazonaws.com</code>. In addition, Route 53 is
able to return up to 4KB of data still using UDP, so using the <code>+tcp</code>
flag may not be necessary.</em></p>

<p>Knowing this, you can do a little bit of troubleshooting on your own.
First, resolve the ELB name to a list of nodes (as A records):</p>

<pre><code>$ dig @ns-942.amazon.com +tcp elb-123456789.us-east-1.elb.amazonaws.com ANY
</code></pre>

<p>The <code>tcp</code> flag is suggested as your ELB could have too many records to fit
inside of a single UDP packet. You also need to perform an <code>ANY</code> query because
Amazon’s nameservers will only return a subset of the nodes otherwise.  Running
this command will give you output that looks something like this (trimmed for
brevity):</p>

<pre><code>;; ANSWER SECTION:
elb-123456789.us-east-1.elb.amazonaws.com. 60 IN SOA ns-942.amazon.com. root.amazon.com. 1376719867 3600 900 7776000 60
elb-123456789.us-east-1.elb.amazonaws.com. 600 IN NS ns-942.amazon.com.
elb-123456789.us-east-1.elb.amazonaws.com. 60 IN A 54.243.63.96
elb-123456789.us-east-1.elb.amazonaws.com. 60 IN A 23.21.73.53
</code></pre>

<p>Now, for each of the <code>A</code> records use e.g. <code>curl</code> to test a connection to
the ELB. Of course, you also want to isolate your test to just the ELB
without connecting to your backends. One final property and little known
fact about ELBs:</p>

<ul>
  <li>The maximum size of the request method (verb) that can be sent through an ELB is <strong>127 characters</strong>. Any larger and the ELB will reply with an <em>HTTP 405 - Method not allowed</em>.</li>
</ul>

<p>This means that we can take advantage of this behavior to test only that
the ELB is responding:</p>

<pre><code>$ curl -X $(python -c 'print "A" * 128') -i http://ip.of.individual.node
HTTP/1.1 405 METHOD_NOT_ALLOWED
Content-Length: 0
Connection: Close
</code></pre>

<p>If you see <code>HTTP/1.1 405 METHOD_NOT_ALLOWED</code> then the ELB is responding
successfully. You might also want to adjust curl’s timeouts to values
that are acceptable to you.</p>

<h1 id="troubleshooting-elbs-using-elbping">Troubleshooting ELBs using elbping</h1>

<p>Of course, doing this can get pretty tedious so I’ve built a tool to
automate this called <a href="https://github.com/heroku/elbping">elbping</a>. It’s
available as a ruby gem, so if you have rubygems then you can install it
by simply doing:</p>

<pre><code>$ gem install elbping
</code></pre>

<p>Now you can run:</p>

<pre><code>$ elbping -c 4 http://elb-123456789.us-east-1.elb.amazonaws.com
Response from 54.243.63.96: code=405 time=210 ms
Response from 23.21.73.53: code=405 time=189 ms
Response from 54.243.63.96: code=405 time=191 ms
Response from 23.21.73.53: code=405 time=188 ms
Response from 54.243.63.96: code=405 time=190 ms
Response from 23.21.73.53: code=405 time=192 ms
Response from 54.243.63.96: code=405 time=187 ms
Response from 23.21.73.53: code=405 time=189 ms
--- 54.243.63.96 statistics ---
4 requests, 4 responses, 0% loss
min/avg/max = 187/163/210 ms
--- 23.21.73.53 statistics ---
4 requests, 4 responses, 0% loss
min/avg/max = 188/189/192 ms
--- total statistics ---
8 requests, 8 responses, 0% loss
min/avg/max = 188/189/192 ms
</code></pre>

<p>Remember, if you see <code>code=405</code> then that means that the ELB is responding.</p>

<h1 id="next-steps">Next Steps</h1>

<p>Whichever method you choose, you will at least know if your ELB’s nodes
are responding or not. Armed with this knowledge, you can either turn
your focus to troubleshooting other parts of your stack or be able to
make a pretty reasonable case to AWS that something is wrong.</p>

<p>Hope this helps!</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Painless instrumentation of Celery tasks using statsd and graphite]]></title>
    <link href="http://www.charleshooper.net/blog/painless-instrumentation-of-celery-tasks-using-statsd-and-graphite/"/>
    <updated>2012-03-12T00:00:00-07:00</updated>
    <id>http://www.charleshooper.net/blog/painless-instrumentation-of-celery-tasks-using-statsd-and-graphite</id>
    <content type="html"><![CDATA[<h1 id="section"></h1>

<p><img src="http://www.charleshooper.net/wp-content/uploads/400px-Steuerstand01-150x150.jpg" alt="" title="A Control Station" />
For one of my clients and side projects, we’ve been working hard to build in application-level metrics to our wide portfolio of services. Among these services is one built on top of the <a href="http://celeryproject.org/">Celery distributed task queue</a>. We wanted a system that required as little configuration as possible to publish new metrics. For this reason, we decided on using <a href="http://codeascraft.etsy.com/2011/02/15/measure-anything-measure-everything/">statsd</a> and <a href="http://graphite.wikidot.com/">graphite</a>. Getting statsd and graphite running was the easy part, but we needed a quick, painless way of adding the instrumentation code for the most basic metrics to our Celery-backed service.</p>

<p>For us, those basic metrics consisted of:</p>

<ul>
  <li>Number of times a worker starts on a specific task</li>
  <li>Number of times a task raises an exception</li>
  <li>Number of times a task completes successfully (no exceptions)</li>
  <li>How long each task takes to complete</li>
</ul>

<p>Since the code to enable these metrics just <em>wraps</em> the code being instrumented it seemed only natural to use a decorator. Below is the code I wrote to do just that.</p>

<p>{% codeblock statsd_instrument.py https://gist.github.com/chooper/2018362 %}</p>

<p>”"”Decorator to quickly add statsd (graphite) instrumentation to Celery
task functions.</p>

<p>With some slight modification, this could be used to instrument just
about any (non-celery) function and be made abstract enough to customize
metric names, etc.</p>

<p>Stats reported include number of times the task was accepted by a worker
(<code>started</code>), the number of successes, and the number of times the task
raised an exception. In addition, it also reports how long the task took
to complete. Usage:</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>@task
@instrument_task
def mytask():
    # do stuff
    pass</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>Please note that the order of decorators is important to Celery. See
http://ask.github.com/celery/userguide/tasks.html#decorating-tasks
for more information.</p>

<p>Uses <code>simple_decorator</code> from
http://wiki.python.org/moin/PythonDecoratorLibrary#Property_Definition</p>

<p>Limitation: Does not readily work on subclasses of celery.tasks.Task
because it always reports <code>task_name</code> as ‘run’
“””</p>

<h1 id="statsd-instrumentation">statsd instrumentation</h1>
<p>from celery import current_app
import statsd</p>

<p>@simple_decorator
def instrument_task(func):
    “"”Wraps a celery task with statsd instrumentation code”””</p>

<pre><code>def instrument_wrapper(*args, **kwargs):
    stats_conn = statsd.connection.Connection(
        host = current_app.conf['STATSD_HOST'],
        port = current_app.conf['STATSD_PORT'],
        sample_rate = 1)

    task_name = func.__name__

    counter = statsd.counter.Counter('celery.tasks.status',stats_conn)
    counter.increment('{task_name}.started'.format(**locals()))

    timer = statsd.timer.Timer('celery.tasks.duration', stats_conn)
    timer.start()

    try:
        ret = func(*args, **kwargs)
    except:
        counter.increment('{task_name}.exceptions'.format(**locals()))
        raise
    else:
        counter.increment('{task_name}.success'.format(**locals()))
        timer.stop('{task_name}.success'.format(**locals()))
        return ret
    finally:
        try:
            del timer
            del counter
            del stats_conn
        except:
            pass

return instrument_wrapper
</code></pre>

<p>def simple_decorator(decorator):
    “"”Borrowed from:
    http://wiki.python.org/moin/PythonDecoratorLibrary#Property_Definition</p>

<pre><code>Original docstring:
This decorator can be used to turn simple functions
into well-behaved decorators, so long as the decorators
are fairly simple. If a decorator expects a function and
returns a function (no descriptors), and if it doesn't
modify function attributes or docstring, then it is
eligible to use this. Simply apply @simple_decorator to
your decorator and it will automatically preserve the
docstring and function attributes of functions to which
it is applied."""
def new_decorator(f):
    g = decorator(f)
    g.__name__ = f.__name__
    g.__module__ = f.__module__ # or celery throws a fit
    g.__doc__ = f.__doc__
    g.__dict__.update(f.__dict__)
    return g
# Now a few lines needed to make simple_decorator itself
# be a well-behaved decorator.
new_decorator.__name__ = decorator.__name__
new_decorator.__doc__ = decorator.__doc__
new_decorator.__dict__.update(decorator.__dict__)
return new_decorator
</code></pre>

<p>{% endcodeblock %}</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Automating Webcam Snapshots and Uploads to Flickr]]></title>
    <link href="http://www.charleshooper.net/blog/automating-webcam-snapshots-and-uploads-to-flickr/"/>
    <updated>2011-03-26T00:00:00-07:00</updated>
    <id>http://www.charleshooper.net/blog/automating-webcam-snapshots-and-uploads-to-flickr</id>
    <content type="html"><![CDATA[<h1 id="section"></h1>

<p>With gardening season right around the corner, one of my desires was to set something up that would allow me to take automated, regular snapshots of some of my plants and upload them to flickr. After a few cumulative hours I finally cobbled together the solution.</p>

<h2 id="taking-the-snapshots">Taking the Snapshots</h2>

<p>The first thing I needed to do was to take snapshots from an installed USB webcam and save them to a directory. This needed to be able to run from a cron script so obviously it needed to work without a GUI and without user-interaction. I read in a <a href="http://www.tldp.org/HOWTO/html_single/Webcam-HOWTO/#COMMAND">Webcam Howto</a> that I could do this using <strong>streamer</strong> so I installed it and wrote a short shell script that would iterate through the video devices installed on my PC and run the snapshot command. You can <a href="https://bitbucket.org/hoop/snapshot/src/98aa5d9a2038/snapshot">view the source of this script here</a>.</p>

<h2 id="uploading-the-photos">Uploading the Photos</h2>

<p>Next I wanted to automatically upload the files to Flickr. At first, I tried using a script I found called uploadr.py which worked OK, but I also wanted to add my photos to a specific set which this script didn’t do. I probably could have extended its functionality, but this script didn’t use or implement the full Flickr API which made this task seem unnecessary.</p>

<p>Instead, I downloaded the <a href="http://stuvel.eu/flickrapi">Python Flickr API from Stuvel</a> and in less than 90 lines I had working code to upload a directory of images to Flickr and add them to a given set. You can view the source to my <a href="https://bitbucket.org/hoop/snapshot/src/98aa5d9a2038/simpleuploadr.py">flickr uploader script</a> here, which I’m calling <a href="https://bitbucket.org/hoop/snapshot">simpleuploadr.py</a> for now.</p>

<h2 id="results">Results</h2>

<p><a href="http://www.flickr.com/photos/hoop2w1/sets/72157626354700156/with/5620672632/">Here are my pretty pictures</a> :) My apologies for the quality, I’m using a really cheap webcam.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Correlating Last Login Dates with Signup Dates from a MMORPG]]></title>
    <link href="http://www.charleshooper.net/blog/correlating-last-login-dates-with-signup-dates-from-a-mmorpg/"/>
    <updated>2011-03-24T00:00:00-07:00</updated>
    <id>http://www.charleshooper.net/blog/correlating-last-login-dates-with-signup-dates-from-a-mmorpg</id>
    <content type="html"><![CDATA[<h1 id="section"></h1>

<p>Yesterday, I wrote a blog post detailing <a href="http://www.charleshooper.net/blog/screen-scraping-search-results-for-information-retrieval/">how I crawled an entire MMORPG’s player database via their search page</a>. Since then, I have been analyzing that data in Minitab and trying to gain some insight into the state of affairs of that game. Today, I’m going to attempt to explain some of that data using statistics and common sense. In particular, we’re going to find out if there’s a relationship between <em>when players join the game *and *when they stop returning</em>.</p>

<h2 id="preparation">Preparation</h2>

<p>I’m new to the statistics software package I’m using, Minitab, and I’m not aware of an easy way to take measurements based on dates. So, my first order of business was to convert dates in the database to an easier metric for analysis, “days since today,” which is simply <em>today’s date</em> minus <em>date x</em>. I did this in my database (MongoDB) prior to export by adding a “last_seen_days” attribute to all documents (records). This attribute is simply the difference between today’s date and the date that the player stopped logging in – measured in days. I then did the same for the signup date. This was quickly done in the MongoDB console in just a few lines:</p>

<pre><code>&gt; var today = new Date();
&gt; var day = 60*60*24*1000;
&gt; db.accounts.find().forEach(function (o) { o.last_seen_days = Math.ceil((today.getTime() - o.last_seen.getTime())/day); db.accounts.save(o); })
&gt; db.accounts.find().forEach(function (o) { o.date_joined_days = Math.ceil((today.getTime() - o.date_joined.getTime())/day); db.accounts.save(o); })
</code></pre>

<h2 id="the-scatterplot">The Scatterplot</h2>

<p>I then exported my data to CSV, loaded it in Minitab, and created a scatterplot between these two attributes. What I got was this:</p>

<p><img src="http://sub-public.s3.amazonaws.com/cch-i/blogimgs/last_seen_vs_signup.jpg" alt="Last Seen Date vs Signup Date" title="Last Seen Date vs Signup Date" /></p>

<p>For the uninitiated, a <strong>scatterplot **is a quick and easy way to visually see if there’s any type of relationship (correlation) between two variables. In this case, I used the signup date as my **independent variable</strong> (<em>x</em>) and the “last seen” date as my <strong>dependent variable</strong> (<em>y</em>). Overall, there is <em>not *any real relationship between the signup date and the last seen date. However</em>,* there are two significant items in this graph that deserve to have some attention brought to them.</p>

<h2 id="observations">Observations</h2>

<p>The first and most obvious item is that <strong>there are not any points above the identity function</strong>. The identity function, or just <em>f(x) = x</em>, is the diagonal line directly across the center of the graph. This makes perfect sense since it’s impossible for a player to have their “last login” occur before they even sign up. I bring this up because this leads into my next observation:</p>

<p><strong>There is a heavier concentration of data points plotted on or directly below the line of the identity function</strong>. For points exactly <em>on</em> the identity function, these are accounts that registered but were never logged into. For accounts *below *the identity function, these should be considered more significant to those who run the game. Why is that? Because, simply put, I believe that these accounts belong to players who went through the effort of joining; They signed up, validated their email address, logged in, and for whatever reason chose not to stick around. This is akin to the “<strong>bounce rate</strong>” so frequently mentioned in the context of web analytics.</p>

<p>It’s possible that these new players didn’t  understand the interface and left, or maybe they thought the game play was too slow, or maybe… this list could go on. What’s important is that some attention is paid here. Some effort should be made to discover why these players are leaving and the number of these players (or almost-players) should be measured, monitored, and analyzed. Decreasing this metric (“bounce rate”) should be a regular goal as these players represent a potential revenue stream for the game’s owner as well as a potential contribution to the game for the rest of the players.</p>

<h2 id="the-histogram">The Histogram</h2>

<p>While, in this case, the scatterplot helped us see that there are a noticeable amount of players who quickly “bounce” after joining the game, this type of graph doesn’t make it particularly easy to measure the magnitude of this phenomena. From observing this behavior, we next want to know how many players are leaving, or what our “bounce rate” is. Instead of first trying to quantitatively define the bounce rate so that we can measure it, it’s probably best if we first take a look at the total distribution of how long players are active for before leaving. For this, we’ll use the histogram of “Days Active”. <em>Days active</em> is simply <em>days since signup</em> minus <em>days since last login</em>.Here’s what we’ve got:</p>

<p><img src="http://sub-public.s3.amazonaws.com/cch-i/blogimgs/days_active_hist.jpg" alt="" title="Histogram of Days Active (excl. lowest-ranked accounts)" /></p>

<p>In this histogram, I excluded the lowest rank from being included in the histogram. I did this because I was more interested in how many potentially-active players were leaving, as opposed to junk accounts. As such, our definition of the <em>bounce rate</em> is already becoming more different than the bounce rate in web analytics.</p>

<p>Each bin (“bar”) in our histogram is 15 days wide. Knowing this, you can see from the histogram that the largest density of days active seems to be about from 15 days to 2.5 months. This chunk, while significant, doesn’t have much to do with our bounce rate mentioned above. What we’re instead interested in is the near-5% of players who become inactive in less than a week.</p>

<h2 id="whats-next">What’s Next?</h2>

<p>If this were my game (it’s not), I would work on defining what level of bounce rate is acceptable and set some goals based on that. I would then look into the large amount of players leaving within the first 2.5 months and try to increase player retention. Finally, I would automate these measurements and have them displaced in a nice administrative dashboard (I’ve always wanted one of those) so that I have to see them all the time.</p>
]]></content>
  </entry>
  
</feed>
