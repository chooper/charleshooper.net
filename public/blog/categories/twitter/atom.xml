<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Twitter | Charles Hooper]]></title>
  <link href="http://www.charleshooper.net/blog/categories/twitter/atom.xml" rel="self"/>
  <link href="http://www.charleshooper.net/"/>
  <updated>2013-10-02T09:34:58-07:00</updated>
  <id>http://www.charleshooper.net/</id>
  <author>
    <name><![CDATA[Charles Hooper]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Twitter vs Erotica: Your Corpora&#8217;s Source Matters]]></title>
    <link href="http://www.charleshooper.net/blog/twitter-vs-erotica-your-corporas-source-matters/"/>
    <updated>2010-10-30T00:00:00-07:00</updated>
    <id>http://www.charleshooper.net/blog/twitter-vs-erotica-your-corporas-source-matters</id>
    <content type="html"><![CDATA[<h1 id="section"></h1>

<p><img src="http://www.charleshooper.net/wp-content/uploads/3998147057_f5dd7ce442-150x150.jpg" alt="Dictionary" title="Dictionary" />
© uair01; some rights reserved.</p>

<p>As a result of my now <a href="http://www.charleshooper.net/blog/rip-booksuggest/">defunct project, BookSuggest</a>, I’ve built a fairly large corpus that has been seeded entirely from Twitter. This corpus weighs in at:</p>

<ul>
  <li>16,680,000 documents (tweets)</li>
  <li>1,970,165 unique (stemmed) words<br />
<em>(Red flag: <a href="http://www.oxforddictionaries.com/page/93">Oxford Dictionary</a> suggests there’s an estimated 250,000 words in the English language. This discrepancy is the result of my failure to filter Tweets based on language, the fact that usernames were included in the count, and the fact that people “make words up.” Also, “haha” becomes one word while “hahahaha” becomes another.)</em></li>
  <li>83,758,872 words total.</li>
</ul>

<p>When I look at these numbers, I often think about how the source documents a corpus/histogram is derived from affects the distribution of its term frequencies. The most obvious example is language. A French corpus will never come close to an English corpus. A less obvious example is subject matter. For example, a corpus derived from English literature will have a different term distribution than a corpus derived from an English medical journal. Common terms will have similar frequencies, but there will be biases towards terms that are domain-specific.</p>

<p>To demonstrate, I scraped the “Erotica” section of <a href="http://www.textfiles.com/">textfiles.com</a> and built a corpus based on the data there. The resulting corpus is composed of:</p>

<ul>
  <li>4,337 documents</li>
  <li>50,709 unique (stemmed) words</li>
  <li>10,413,715 words total.</li>
</ul>

<h2 id="notes-on-term-counting">Notes on Term Counting</h2>

<ul>
  <li>Words that had a length of <em>less than 4 characters</em> were <strong>discarded</strong></li>
  <li>Words were then <em>stemmed</em> using the <a href="http://tartarus.org/~martin/PorterStemmer/">Porter Stemming algorithm</a></li>
  <li>There may be some slight differences between how words were counted in both corpora, based on minor programming differences</li>
</ul>

<h2 id="the-data">The Data</h2>

<p>Finally, here are the term frequencies with the obvious domain-specific terms in bold:</p>

<p><strong>Corpus Seeded from Twitter</strong></p>

<p>![Counts of Top 20 Terms from Twitter Corpus][6]  </p>

<ol>
  <li>that (0.84%)</li>
  <li>just (0.70%)</li>
  <li>with (0.69%)</li>
  <li>thi (0.68%)</li>
  <li>have (0.65%)</li>
  <li>your (0.61%)</li>
  <li>like (0.56%)</li>
  <li>love (0.54%)</li>
  <li><strong>follow (0.45%)</strong></li>
  <li>what (0.44%)</li>
  <li>from (0.36%)</li>
  <li>haha (0.35%)</li>
  <li>good (0.34%)</li>
  <li>para (0.34%)</li>
  <li>will (0.32%)</li>
  <li>when (0.30%)</li>
  <li>know (0.30%)</li>
  <li>want (0.30%)</li>
  <li>about 0.30%)</li>
  <li>make (0.30%)</li>
</ol>

<p><strong>Corpus Seeded from Erotica</strong></p>

<p>![Counts of Top 20 Terms from Erotica Corpus][7]  </p>

<ol>
  <li>that (1.83%)</li>
  <li>with (1.42%)</li>
  <li>into (0.76%)</li>
  <li>down (0.70%)</li>
  <li>then (0.66%)</li>
  <li>back (0.66%)</li>
  <li>from (0.65%)</li>
  <li>thi (0.65%)</li>
  <li>hand (0.64%)</li>
  <li>were (0.59%)</li>
  <li>look (0.58%)</li>
  <li>have (0.58%)</li>
  <li><strong>cock (0.57%)</strong></li>
  <li>like (0.57%)</li>
  <li>over (0.57%)</li>
  <li>thei (0.56%)</li>
  <li>your (0.56%)</li>
  <li>what (0.55%)</li>
  <li>said (0.55%)</li>
  <li>could (0.54%)</li>
</ol>

<p>You’ll note that the Twitter corpus had a heavy bias towards the term “<em>follow</em>” whereas the Erotica corpus shows an overwhelming use of the term “<em>cock</em>” (Writers: Use synonyms.)</p>

<table>
  <tbody>
    <tr>
      <td>[6]: http://chart.apis.google.com/chart?chxl=0:</td>
      <td>that</td>
      <td>just</td>
      <td>with</td>
      <td>thi</td>
      <td>have</td>
      <td>your</td>
      <td>like</td>
      <td>love</td>
      <td>follow</td>
      <td>what</td>
      <td>from</td>
      <td>haha</td>
      <td>good</td>
      <td>para</td>
      <td>will</td>
      <td>when</td>
      <td>know</td>
      <td>want</td>
      <td>about</td>
      <td>make&amp;chxr=0,0,703297&amp;chxt=x&amp;chbh=a,4,10&amp;chs=600x200&amp;cht=bvg&amp;chco=4D89F9&amp;chds=0,703297&amp;chd=t:703297,582988,581346,573197,547218,513823,467673,455264,378187,367112,302254,296974,286671,283887,272176,254419,252303,251673,251325,248572&amp;chtt=Counts of Top 20 Terms from Twitter Corpus</td>
    </tr>
    <tr>
      <td>[7]: http://chart.apis.google.com/chart?chxl=0:</td>
      <td>that</td>
      <td>with</td>
      <td>into</td>
      <td>down</td>
      <td>then</td>
      <td>back</td>
      <td>from</td>
      <td>thi</td>
      <td>hand</td>
      <td>were</td>
      <td>look</td>
      <td>have</td>
      <td>cock</td>
      <td>like</td>
      <td>over</td>
      <td>thei</td>
      <td>your</td>
      <td>what</td>
      <td>said</td>
      <td>could&amp;chxr=0,0,190543&amp;chxt=x&amp;chbh=a,4,10&amp;chs=600x200&amp;cht=bvg&amp;chco=F889F9&amp;chds=0,190543&amp;chd=t:190543,148204,78688,72452,69045,68642,68164,67998,66826,61787,60236,60179,59622,59357,58856,58760,57851,57670,57348,55739&amp;chtt=Counts of Top 20 Terms from Erotica Corpus</td>
    </tr>
  </tbody>
</table>

<h2 id="practical-reasons-why-this-is-important">Practical Reasons Why This Is Important</h2>

<p>This is important because if I were to build a domain-specific search-engine, I would be better off  seeding my corpus from domain-specific content. If I don’t, my relevance (tf-idf) scores will be inaccurate. For example, an Erotica-specific search engine should decrease the weight for the term “<em>cock</em>” strictly because it has a very high document frequency and is therefore less-significant. Meanwhile, a Twitter-specific search engine should discount the weight of “<em>follow</em>.”</p>

<h2 id="conclusion">Conclusion</h2>

<p>To conclude, the subject matter of a document set will create a bias towards domain-specific terms in the document set’s histogram of term frequencies. If you are calculating relevance for any particular document set, you should use a corpus derived from that document set. In other words, if you can, try not to re-use your corpora!</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How I Made Money Spamming Twitter with Contextual Book Suggestions]]></title>
    <link href="http://www.charleshooper.net/blog/how-i-made-money-spamming-twitter-with-contextual-book-suggestions/"/>
    <updated>2010-08-13T00:00:00-07:00</updated>
    <id>http://www.charleshooper.net/blog/how-i-made-money-spamming-twitter-with-contextual-book-suggestions</id>
    <content type="html"><![CDATA[<h1 id="section"></h1>

<p>Two winters ago I left a position as a system administrator that was paying pretty well and moved cross-country to a region with less jobs than where I moved from. Three months later, I was still unemployed, broke, and bored. I was talking to my good friend <a href="http://japherwocky.posterous.com/">Japhy</a> on IRC one day and he was explaining to me how the <a href="http://en.wikipedia.org/wiki/Tf–idf">tf-idf algorithm</a> works. For reasons involving boredom more than any other reason, I dreamed up an idea: <strong>I would write software that would take a given document and generate book suggestions based on its content.</strong> </p>

<p>I think that most programmers would agree with me that we put in longer hours on code when we’re not working for anybody. We don’t stop learning, either. To us, <em>unemployment is a brief sprint of academia</em> spent in our home office, the local coffee shop, or our parent’s house. 
My imagination dreamed up this fairly straightforward process:</p>

<ol>
  <li>Take a given document and calculate tf-idf scores on all terms</li>
  <li>Select X number of the highest scoring terms</li>
  <li>Pass these high-scoring terms to an Amazon ItemSearch query</li>
  <li>Receive a list of recommended books (with URLs) from Amazon</li>
</ol>

<p>I had already written multiple Twitter bots by this time so I decided to just use some of my existing code to poll Twitter’s search API. Essentially, the “documents” I mentioned above were actually tweets containing the terms “book” or “books.” Two and a half days later I had a working prototype that could generate a book recommendation from a given tweet. It was at this time that I added steps 5 and 6:</p>

<p>Tag URLs returned from Amazon’s ItemSearch with an affiliate ID; and<br />
Reply to the tweeting user with their new book suggestion</p>

<p>Four months later and <strong>I had generated over $7,000 in sales for Amazon with over $400 commission for myself</strong>. Obviously, the commission I was making wasn’t livable but it was a nice addition to my then-depleting savings. Had I decided to scale out my operation, I could have made much more. My benchmark is at four months because that’s how long I went before being <em>suspended</em>. My conversion rate? <strong>0.13%</strong>! While seemingly low, this number is very high when compared to email spam. However, it’s important to note that email spam is subject to various filtering technologies. 
<img src="http://cdn.subversity.net/blog_imgs/twitter-spam-earnings.png" alt="twitter-spam-earnings.png" title="twitter-spam-earnings.png" /> 
A fair amount of the time I share this story, people are more impressed with the fact that I went 4 months before getting suspended. The truth is, I had a lot of throttling built into my spam bot. The factors I think are important to point out are:</p>

<ol>
  <li>Twitter’s Terms of Service at that time basically only outlawed “unsolicited replies,” nothing that really attacked targeted spam.</li>
  <li>Twitter’s anti-spam stance did exist in writing (only in the help site,) but I do not think they were actively enforcing their policies.</li>
  <li>My recommendations were contextual and, unless you looked at my bot’s timeline and tweet count, looked legitimate (most of the time.) In other words, I was tweeting <em>book suggestions</em> to people who were already talking about <em>books</em>.</li>
  <li>I recorded the usernames of everyone I sent recommendations to and would only @mention them once.</li>
  <li>I built in a “chattiness” rate limiting function. This was to distribute my spam throughout a whole hour (due to Twitter’s rate limiting) more than anything.</li>
</ol>

<p><img src="http://cdn.subversity.net/blog_imgs/twitter-suspended.png" alt="twitter-suspended.png" title="twitter-suspended.png" /> </p>

<p>While it only lasted a short while, I had alot of fun and made a little bit of money spamming Twitter. </p>

<p>The second re-incarnation of this project turned into <a href="http://www.charleshooper.net/twitter/">BookSuggest, a website for recommending books based on a person’s Twitter feed</a>. I haven’t put alot of effort into promoting it, but my conversion rate is much lower now that I’m not pushing the links in anyone’s face. </p>

<p>Try it out and comment here – what did BookSuggest tell YOU to read?</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Drinking from the Gardenhose, with Graphs!]]></title>
    <link href="http://www.charleshooper.net/blog/drinking-from-the-gardenhose-with-graphs/"/>
    <updated>2009-12-23T00:00:00-08:00</updated>
    <id>http://www.charleshooper.net/blog/drinking-from-the-gardenhose-with-graphs</id>
    <content type="html"><![CDATA[<h1 id="section"></h1>

<p>My Tweet harvester stats look something like this. Base “sample” streaming level. Times are GMT or UTC.</p>

<p><img src="http://www.charleshooper.net/wp-content/uploads/bowserjr.plumata.net-harvester-300x171.png" alt="a" /><br />
<img src="http://www.charleshooper.net/wp-content/uploads/bowserjr.plumata.net-munin_rat-300x164.png" alt="b" /><br />
<img src="http://www.charleshooper.net/wp-content/uploads/magichat.plumata.net-mysql_slo-300x164.png" alt="c" /><br />
<img src="http://www.charleshooper.net/wp-content/uploads/magichat.plumata.net-mysql_que-300x207.png" alt="d" /><br />
<img src="http://www.charleshooper.net/wp-content/uploads/bowserjr.plumata.net-if_eth0-d-300x164.png" alt="e" /><br />
<img src="http://www.charleshooper.net/wp-content/uploads/bowserjr.plumata.net-munin_que-300x164.png" alt="f" /></p>

<p>Edit: I’m actually on spritzer, or whatever the base access level is. However, “drinking from the gardenhose” is a much cooler title.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Drinking from the Gardenhose, cont.]]></title>
    <link href="http://www.charleshooper.net/blog/drinking-from-the-gardenhose-cont/"/>
    <updated>2009-12-16T00:00:00-08:00</updated>
    <id>http://www.charleshooper.net/blog/drinking-from-the-gardenhose-cont</id>
    <content type="html"><![CDATA[<h1 id="section"></h1>

<p>Previously I blogged about a <a href="http://subversity.net/drinking-from-the-gardenhose">storage/database bottleneck causing dropped connections while utilizing Twitter’s streaming API</a>. 
I’m happy to report that the switch from <em>sqlite to MySQL</em> resulted in an immediate increase in <strong>throughput</strong>. I went from processing <strong>~5 updates/second</strong> to processing just over <strong>11 updates/second</strong>, almost doubling my capacity.</p>

<p>I also saw great improvement in terms of <strong>CPU usage</strong> as well. Previously, I was pegging my CPU at <strong>100% usage</strong>. Since the switch to MySQL, which runs on another (similarly spec’d) host, I now use less than <strong>5% CPU</strong> on the stream listening/processing host and less than <strong>10% CPU</strong> on the MySQL host with the same dataset as before. I believe that parallelizing my code even further would allow me to take greater advantage of my resources and achieve higher throughput.</p>

<p>Resolving this issue has allowed me to turn my focus back on what I originally started this project for: Building a large enough corpus to do accurate <a href="http://en.wikipedia.org/wiki/Tf–idf">Tf-idf scoring</a>.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Drinking from the Gardenhose]]></title>
    <link href="http://www.charleshooper.net/blog/drinking-from-the-gardenhose/"/>
    <updated>2009-12-15T00:00:00-08:00</updated>
    <id>http://www.charleshooper.net/blog/drinking-from-the-gardenhose</id>
    <content type="html"><![CDATA[<h1 id="section"></h1>

<p>Lately I’ve been working on some code to process tweets coming out of the <em>Gardenhose</em>. The <strong>Gardenhose</strong> is the name given to the lowest level of access to <a href="http://apiwiki.twitter.com/Streaming-API-Documentation">Twitter’s streaming API</a>. Oddly enough however, my connections to the Twitter API were dropping, causing my program to hang and my CPU to remain maxed out at 100% usage. 
After much debugging and banging my head on my desk, <a href="http://github.com/joshthecoder/tweepy">joshthecoder (creator of tweepy)</a> and uuid (#twitterapi / freenode) pointed out that I may be filling some type of buffer or queue, causing this condition. They suggested that my current workflow of <em>“receive tweet, process, store to DB”</em> all in the same thread might be too slow. After removing key parts of my processing code, what once took 100% now only took 1-3% CPU, I was receiving a much higher volume of tweets, and my connections were no longer dropping. This meant that the culprit of my general slowness was the <strong>sqlite database</strong>. 
Basically, all I was doing with these tweets was tokenizing them, tallying up their contents, and then updating a corpus (stored in a sqlite database) that would track how many tweets each word appeared in. Having this data would allow me to perform accurate <a href="http://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html">Tf-idf</a> scoring later on. <a href="http://codepad.org/8OkD5AGM">You can see the original (and admittedly not-very-well documented) version of this program here (Python)</a>. 
Fixing this problem is a two-step solution:</p>

<ol>
  <li>Break off the processing code into another thread</li>
  <li>Replace the sqlite database with something faster</li>
</ol>

<p>The first step, moving the process code into its own thread, is a pretty easy feat. In fact, I’m testing my proof-of-concept as I write. The second step, however, is going to require alot more research and thought. In my proof-of-concept, I place Status (Tweet) objects into a queue which another thread then processes. Using sqlite, my queue never gets any smaller, as you can see below.</p>

<p><code>
$ ./bootstrap-stream.py
Streaming timelines...
Tue Dec 15 10:56:01 2009 | Queue: 93 items
Tue Dec 15 10:56:08 2009 | Queue: 183 items
Tue Dec 15 10:56:13 2009 | Queue: 278 items
...
Tue Dec 15 10:58:02 2009 | Queue: 1859 items
</code></p>

<p>My next step will probably be to test storing these items in a MySQL (InnoDB) database, simply because it’s easy enough to swap out the sqlite module with the MySQLdb module in Python, although I  have a feeling that what I really need is a key-value store. Speaking of which, does any one have any recommendations as far as databases, hash tables, or key-value stores go? Bonus points if I can replicate the data.</p>
]]></content>
  </entry>
  
</feed>
