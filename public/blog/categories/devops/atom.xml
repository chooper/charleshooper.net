<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: DevOps | Charles Hooper]]></title>
  <link href="http://www.charleshooper.net/blog/categories/devops/atom.xml" rel="self"/>
  <link href="http://www.charleshooper.net/"/>
  <updated>2015-11-24T15:27:46-08:00</updated>
  <id>http://www.charleshooper.net/</id>
  <author>
    <name><![CDATA[Charles Hooper]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Briefly: Operator Requirements]]></title>
    <link href="http://www.charleshooper.net/blog/briefly-operator-requirements/"/>
    <updated>2015-11-23T10:49:00-08:00</updated>
    <id>http://www.charleshooper.net/blog/briefly-operator-requirements</id>
    <content type="html"><![CDATA[<h1 id="section"></h1>

<p>On any given day, there are a number of people discussing user requirements and
prioritizing the work ahead of them based on them. There’s an
oft-underrepresented group of users however and those are your operators.
Typically, the set of things needed by your operators are buried in your
project’s list of “non-functional requirements”, if at all.</p>

<p>In this brief, I would like to provide you with a de facto set of “operator
requirements” for your project. This list is likely incomplete and I’m
discovering more every day. I may update this post from time to time to add
things or clarify them as I journey towards understanding.</p>

<p>An application that satisfies these requirements will be more scalable, easier
to operate, and likely have a lower Mean Time To Recovery than an application
that does not.</p>

<ol>
  <li>
    <p>In general you should strive to adhere to <a href="http://12factor.net/">12factor</a> if you’re building a web
  application. 12factor creates a clean contract between your application and
  the operating system, enables simpler deployments, and results in applications
  that are mostly horizontally scalable by default. If you cannot adhere to
  12factor, then I would challenge you to borrow as much of it as you can before
  discounting the whole 12factor methodology.</p>
  </li>
  <li>
    <p>Your application should have plenty of <a href="http://www.charleshooper.net/blog/briefly-logs/">logging and follow best
  practices</a>.</p>
  </li>
  <li>
    <p>Your application should also emit metrics that create some sense of
  understanding of what the system is doing.</p>
  </li>
  <li>
    <p>Your application’s services should have <a href="http://www.charleshooper.net/blog/briefly-health-checks/">health checks</a>. The health checks
  should return HTTP 2xx or 3xx when the service is healthy and HTTP 5xx when
  it is not. The response body should contain an explanation or identifier that
  will allow the operator to determine why the health check failed to aid in
  incident recovery.</p>
  </li>
  <li>
    <p>Your application should use <a href="https://brandur.org/request-ids">unique request IDs</a> and add them to their
  logging contexts (see logging).</p>
  </li>
  <li>
    <p>Your application should support credential rotation. Any given secret,
  whether it’s a password, API key, SSL private key, or otherwise, should be
  changeable with minimal disruption to the service. This should be exercised
  often to ensure it works as designed.</p>
  </li>
  <li>
    <p>Your application should provide operators with <a href="http://blog.travis-ci.com/2014-03-04-use-feature-flags-to-ship-changes-with-confidence/">toggles or feature flags</a> —
  parameters that allow the operators or the system itself to turn off bits of
  functionality when the system is degraded.</p>
  </li>
  <li>
    <p>Your application should put external resources behind <a href="https://engineering.heroku.com/blogs/2015-06-30-improved-production-stability-with-circuit-breakers/">circuit breakers</a>.
  Circuit breakers allow your app to continue operating (albeit in a degraded
  state) when an external resource is unavailable instead of taking your
  application offline.</p>
  </li>
  <li>
    <p>Your application should be <a href="http://12factor.net/disposability">disposable and restartable</a>; this means that
  it’s restartable on the same instance or a new instance) after a crash and
  should crash in an automatically recoverable state. If your crash is not
  automatically recoverable, it should scream! In addition, your application
  should gracefully complete existing work such as HTTP requests or jobs it
  picked up from a task queue. In the case of long running jobs, your application
  should be able to abandon the work to have it picked up by another worker or
  node.</p>
  </li>
</ol>

<p>These are just a start but these requirements should be imported into your project’s
requirements and prioritized with maintainability in mind. By doing so, your
application will be more scalable, easier to operator, and have a lower Mean
Time To Recovery than application that don’t satisfy these requirements.</p>

<p>Do you feel like I missed anything? What else would you recommend?</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Briefly: Health Checks]]></title>
    <link href="http://www.charleshooper.net/blog/briefly-health-checks/"/>
    <updated>2015-11-08T01:23:00-08:00</updated>
    <id>http://www.charleshooper.net/blog/briefly-health-checks</id>
    <content type="html"><![CDATA[<h1 id="section"></h1>

<p>Health checks are specially defined endpoints or routes in your application
that allow external monitors to determine the health of your web application.
They are so important to production health that I consider them the “13th
factor” in <a href="http://12factor.net/">12factor</a>.</p>

<p>If an application is healthy it will return a HTTP 2xx or 3xx status code and
when it is not it will return an HTTP 5xx status code.</p>

<p>This type of output allows load balancers to remove unhealthy instances from
its rotation but can also be used to alert an operator or even automatically
replace the instance.</p>

<p>In order to implement proper health checks, your application’s health checks
should:</p>

<ol>
  <li>
    <p>Return a HTTP 2xx or 3xx status code when healthy</p>
  </li>
  <li>
    <p>Return a HTTP 5xx status code when not healthy</p>
  </li>
  <li>
    <p>Include the reason why the check failed in the response body</p>
  </li>
  <li>
    <p><a href="http://www.charleshooper.net/blog/briefly-logs/">Log the requests and their results</a> along with Request IDs</p>
  </li>
  <li>
    <p>Not have any side effects</p>
  </li>
  <li>
    <p>Be lightweight and fast</p>
  </li>
</ol>

<p>If you implement health checks in your application following this advice,
you’ll have a more resilient, monitorable, and manageable application.</p>

<p>How about you all? Is there anything you would add?</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Briefly: Logs]]></title>
    <link href="http://www.charleshooper.net/blog/briefly-logs/"/>
    <updated>2015-10-29T05:47:00-07:00</updated>
    <id>http://www.charleshooper.net/blog/briefly-logs</id>
    <content type="html"><![CDATA[<h1 id="section"></h1>

<p>Recently I was asked by another engineer what information I expect to be able
to find in logs. For this, I mostly agree with <a href="http://dev.splunk.com/view/logging-best-practices/SP-CAAADP6">Splunk’s best practices</a> but
I have some additional advice I want to provide. I’ll end up regurgitating some
of Splunk’s recommendations anyway.</p>

<ol>
  <li>
    <p>Your logs should be human readable. This means logging in text (no binary
logging) and in a format that can be read by angry humans. Splunk recommends
key-value pairs (e.g. <code>at=response code=200 bytes=1024</code>) since it makes
Splunking easy, but I don’t have a strong enough opinion to evangelize that.
Some folks advocate for logging in JSON but I don’t actually find JSON to be
very readable.</p>

    <p>Edit: Someone pointed out to me that this isn’t ideal when you have a large
amount of logs. They prefered sending JSON logs to a service like ElasticSearch
but I think also sending key-value pairs to Splunk is also reasonable at some
scale.</p>
  </li>
  <li>
    <p>Every log line should include a timestamp. The timestamp should be human
readable and in a standard format such as <a href="https://tools.ietf.org/html/rfc3339">RFC 3339</a>/<a href="https://en.wikipedia.org/wiki/ISO_8601">ISO 8601</a>.
Finally, even though the above specs include a timezone offset, timestamps
should be stated in UTC time whenever possible.</p>
  </li>
  <li>
    <p>Every log line should include a unique identifier for the work being
performed. In web applications and APIs, for example, this would be a
request ID. The combination of a unique ID and timestamp allows for developers
and operators to trace the execution of a single work unit.</p>
  </li>
  <li>
    <p>More is more. While I don’t particularly enjoy reading logs, I have always
been more happy when an application logs more information than I need versus
when an application doesn’t log enough information. Be verbose and log
everything.</p>
  </li>
  <li>
    <p>Make understanding the code path of a work unit easy. This means logging
file names, class names, function or method names, and so on. When sensible,
include the arguments to these things as well.</p>
  </li>
  <li>
    <p>Use one line per event. Multi-line events are bad because they are difficult
to grep or Splunk. Keep everything on one log line but feel free to log
additional events. An exception to this rule might be tracebacks (see what I
did there?)</p>
  </li>
  <li>
    <p>Log to stdout if you’re following <a href="http://12factor.net/">12factor</a> otherwise log to syslog. Do not
write your own log files! By writing your own log files, you are either
taking log rotation off the table or signing yourself up to support exciting
requirements like re-opening logs on SIGHUP (let’s not go there).</p>
  </li>
  <li>
    <p>Last but not least: <strong>Don’t write your own logging library!</strong> Chances are
there already exists a well thought-out and standard library available in
your application’s language or framework. Please use it!</p>
  </li>
</ol>

<p>So those are my recommendations about logs. What else would you recommend?</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Troubleshooting ELBs with elbping]]></title>
    <link href="http://www.charleshooper.net/blog/troubleshooting-elbs-with-elbping/"/>
    <updated>2013-10-19T10:49:00-07:00</updated>
    <id>http://www.charleshooper.net/blog/troubleshooting-elbs-with-elbping</id>
    <content type="html"><![CDATA[<h1 id="section"></h1>

<p>Troubleshooting ELBs can be pretty painful at times because they are
largely a black box. There aren’t many metrics available, and the ones
that do exist are aggregated across all of the nodes of an ELB. This can
be troublesome at times, for example when only a subset of an ELB’s
nodes are degraded.</p>

<h1 id="elb-properties">ELB Properties</h1>

<p>ELBs have some interesting properties. For instance:</p>

<ul>
  <li>ELBs are made up of 1 or more nodes</li>
  <li>These nodes are published as A records for the ELB name</li>
  <li>These nodes can fail, or be shut down, and connections will <em>not</em> be closed gracefully</li>
  <li>It often requires a good relationship with Amazon support ($$$) to get someone to dig into ELB problems</li>
</ul>

<p><em>NOTE: Another interesting property but slightly less pertinent is that
ELBs were not designed to handle sudden spikes of traffic. They
typically require 15 minutes of heavy traffic before they will scale up
or they can be pre-warmed on request via a support ticket</em></p>

<h1 id="troubleshooting-elbs-manually">Troubleshooting ELBs (manually)</h1>

<p><strong>Update:</strong> <em>Since writing this blog post, AWS has since migrated all
ELBs to use Route 53 for DNS. In addition, all ELBs now have a
<code>all.$elb_name</code> record that will return the full list of nodes for the
ELB. For example, if your ELB name is
<code>elb-123456789.us-east-1.elb.amazonaws.com</code>, then you would get the full
list of nodes by doing something like <code>dig
all.elb-123456789.us-east-1.elb.amazonaws.com</code>. In addition, Route 53 is
able to return up to 4KB of data still using UDP, so using the <code>+tcp</code>
flag may not be necessary.</em></p>

<p>Knowing this, you can do a little bit of troubleshooting on your own.
First, resolve the ELB name to a list of nodes (as A records):</p>

<pre><code>$ dig @ns-942.amazon.com +tcp elb-123456789.us-east-1.elb.amazonaws.com ANY
</code></pre>

<p>The <code>tcp</code> flag is suggested as your ELB could have too many records to fit
inside of a single UDP packet. You also need to perform an <code>ANY</code> query because
Amazon’s nameservers will only return a subset of the nodes otherwise.  Running
this command will give you output that looks something like this (trimmed for
brevity):</p>

<pre><code>;; ANSWER SECTION:
elb-123456789.us-east-1.elb.amazonaws.com. 60 IN SOA ns-942.amazon.com. root.amazon.com. 1376719867 3600 900 7776000 60
elb-123456789.us-east-1.elb.amazonaws.com. 600 IN NS ns-942.amazon.com.
elb-123456789.us-east-1.elb.amazonaws.com. 60 IN A 54.243.63.96
elb-123456789.us-east-1.elb.amazonaws.com. 60 IN A 23.21.73.53
</code></pre>

<p>Now, for each of the <code>A</code> records use e.g. <code>curl</code> to test a connection to
the ELB. Of course, you also want to isolate your test to just the ELB
without connecting to your backends. One final property and little known
fact about ELBs:</p>

<ul>
  <li>The maximum size of the request method (verb) that can be sent through an ELB is <strong>127 characters</strong>. Any larger and the ELB will reply with an <em>HTTP 405 - Method not allowed</em>.</li>
</ul>

<p>This means that we can take advantage of this behavior to test only that
the ELB is responding:</p>

<pre><code>$ curl -X $(python -c 'print "A" * 128') -i http://ip.of.individual.node
HTTP/1.1 405 METHOD_NOT_ALLOWED
Content-Length: 0
Connection: Close
</code></pre>

<p>If you see <code>HTTP/1.1 405 METHOD_NOT_ALLOWED</code> then the ELB is responding
successfully. You might also want to adjust curl’s timeouts to values
that are acceptable to you.</p>

<h1 id="troubleshooting-elbs-using-elbping">Troubleshooting ELBs using elbping</h1>

<p>Of course, doing this can get pretty tedious so I’ve built a tool to
automate this called <a href="https://github.com/heroku/elbping">elbping</a>. It’s
available as a ruby gem, so if you have rubygems then you can install it
by simply doing:</p>

<pre><code>$ gem install elbping
</code></pre>

<p>Now you can run:</p>

<pre><code>$ elbping -c 4 http://elb-123456789.us-east-1.elb.amazonaws.com
Response from 54.243.63.96: code=405 time=210 ms
Response from 23.21.73.53: code=405 time=189 ms
Response from 54.243.63.96: code=405 time=191 ms
Response from 23.21.73.53: code=405 time=188 ms
Response from 54.243.63.96: code=405 time=190 ms
Response from 23.21.73.53: code=405 time=192 ms
Response from 54.243.63.96: code=405 time=187 ms
Response from 23.21.73.53: code=405 time=189 ms
--- 54.243.63.96 statistics ---
4 requests, 4 responses, 0% loss
min/avg/max = 187/163/210 ms
--- 23.21.73.53 statistics ---
4 requests, 4 responses, 0% loss
min/avg/max = 188/189/192 ms
--- total statistics ---
8 requests, 8 responses, 0% loss
min/avg/max = 188/189/192 ms
</code></pre>

<p>Remember, if you see <code>code=405</code> then that means that the ELB is responding.</p>

<h1 id="next-steps">Next Steps</h1>

<p>Whichever method you choose, you will at least know if your ELB’s nodes
are responding or not. Armed with this knowledge, you can either turn
your focus to troubleshooting other parts of your stack or be able to
make a pretty reasonable case to AWS that something is wrong.</p>

<p>Hope this helps!</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Intro to Operations: Metrics Collection]]></title>
    <link href="http://www.charleshooper.net/blog/intro-to-ops-metrics-collection/"/>
    <updated>2013-05-07T14:51:00-07:00</updated>
    <id>http://www.charleshooper.net/blog/intro-to-operations-metrics-collection</id>
    <content type="html"><![CDATA[<h1 id="section"></h1>

<p><em>I’m writing a series of blog posts for managers and other people
without an operations background in order to introduce certain best
practices regarding Operations. For the rest of the blog posts, please
visit the <a href="/blog/intro-to-ops-for-startups/">introductory Intro to Operations</a> blog post!</em></p>

<p>Collecting metrics is another area that many early stage startups seem
to overlook even though it is probably one of the most important things
they can do. By metrics collection, I am referring to the gathering and
storing of various metrics at several different levels. As John Allspaw
identifies them in <a href="http://www.amazon.com/Web-Operations-Keeping-Data-Time/dp/1449377440">Web Operations: Keeping the Data on Time</a>, they
are:</p>

<ul>
  <li><strong>High-level business and application metrics</strong> (e.g. user sign-ups)</li>
  <li><strong>Feature-specific application-level metrics</strong> (e.g. widgets processed)</li>
  <li>
    <p><strong>Systems and service-level metrics</strong> (e.g. server load or database queries per second)</p>

  </li>
</ul>

<p>You’ll note that there are two levels of “application-level” metrics.
The higher-level application metrics are mostly those that can be tied
to business objectives, while the other category of application metrics
are generally more feature specific.</p>

<p>Benefits incurred by collecting these metrics are plentiful. For one,
having quick access to these metrics is helpful during troubleshooting
and incident response. For example, I was once hired under contract to
look into why a certain company’s API was unreliable for the previous
few months. At least once per day, this company’s API would time out and
not respond to client requests. After enabling basic metrics collection
for the servers and services used by the API, it very quickly became
obvious that the database servers were reaching their connection limits
which was preventing the API from retrieving records from the database.
Not only was this problem identified very quickly, but later on we were
able to look back at our metrics data to assess how close to our limits
we were getting.</p>

<p>Another benefit is that you can integrate the metrics into your
<a href="/blog/intro-to-ops-availability-monitoring-alerting/">Availability monitoring</a> system to be alerted when metrics surpass
some threshold or change significantly. Not only that, but analyzing
these metrics will allow you to manage your capacity intelligently and
build a business case to justify infrastructure expenditures. Finally,
analyzing these metrics will also give you insight into your
application, how it’s used, and your business.</p>

<p>How you go about collecting and storing these metrics is up to you. Many
engineers might be tempted to build their own solution; however, there
are many open source and third party software packages that you may find
helpful. Key considerations when choosing which package or packages to
use are:</p>

<ul>
  <li>The ability to add new, custom metrics</li>
  <li>Configurable resolution/storage trade-off</li>
  <li>Integration with availability monitoring and alerting systems</li>
  <li>Graphing/visualization</li>
</ul>

<p>If your startup doesn’t have any metrics then you should start
collecting them now. The visualization will help you in the short run
and the historical data will help you in the long run.</p>

]]></content>
  </entry>
  
</feed>
